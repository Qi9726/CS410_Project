{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ZuWZCey-Oo",
        "outputId": "a95a9610-f6f5-428e-d0c6-1dff05ec707c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ntscraper\n",
            "  Downloading ntscraper-0.3.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ntscraper) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from ntscraper) (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->ntscraper) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ntscraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ntscraper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ntscraper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ntscraper) (2023.11.17)\n",
            "Installing collected packages: ntscraper\n",
            "Successfully installed ntscraper-0.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install ntscraper\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install clean-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggWsc7j-S06D",
        "outputId": "c7bd5c3b-ace3-4b28-ea72-9cac49acbdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0 (from clean-text)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.12)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=84909dd2f18b44a93e6cab07ce20cc52521b704307713f54d20f0e0a35346592\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, ftfy, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ntscraper import Nitter\n",
        "from cleantext import clean\n"
      ],
      "metadata": {
        "id": "lho4S1LPzYOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraper = Nitter()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx47h-6Szhj-",
        "outputId": "da1bad10-186c-4f91-9c59-a9f8d369e7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing instances: 100%|██████████| 29/29 [00:58<00:00,  2.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets(name,modes,num):\n",
        "    tweets = scraper.get_tweets(name,mode = modes,number=num, language = \"en\", filters = [\"nativeretweets\"])\n",
        "    final_tweets = []\n",
        "    for tweet in tweets['tweets']:\n",
        "        data = [tweet['link'], tweet['text'],tweet['date'],tweet['stats']['likes'],tweet['stats']['quotes'],tweet['stats']['comments'],tweet['stats']['retweets']]\n",
        "        final_tweets.append(data)\n",
        "    data = pd.DataFrame(final_tweets,columns= ['link','text','date','likes','quotes','comments','retweets'])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "9te7aTQfBuOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPL,MSFT,AMZN,NVDA,GOOGL,META,TSLA,\n",
        "Compare AI"
      ],
      "metadata": {
        "id": "27EmCwawN94w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "data = get_tweets('META','hashtag',20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5c2RoRDAUgE",
        "outputId": "eeaadbb5-b435-4083-ed33-8d5f79f6a68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:No instance specified, using random instance https://nitter.tinfoil-hat.net\n",
            "INFO:root:Current stats for META: 5 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 5 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 7 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 9 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 13 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 15 tweets, 0 threads...\n",
            "INFO:root:Current stats for META: 20 tweets, 0 threads...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)\n",
        "data['text'] = data['text'].apply(clean)\n"
      ],
      "metadata": {
        "id": "TF7IwTWNQ_BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    hashtag = ['META','MSFT',\"NVDA\",\"AMZN\",\"TSLA\",\"IBM\"]\n",
        "    data = pd.DataFrame()\n",
        "\n",
        "    for i in hashtag:\n",
        "        update_data = get_tweets(i,'hashtag',10)\n",
        "        data = pd.concat([data, update_data], ignore_index=True)\n",
        "\n",
        "    data['text'] = data['text'].str.replace(\n",
        "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)\n",
        "    data.to_csv('tweets.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn_s5Lz4rTt1",
        "outputId": "80380b88-bf0f-411d-996b-db925add7a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:No instance specified, using random instance https://n.opnxng.com\n",
            "INFO:root:Current stats for META: 4 tweets, 0 threads...\n",
            "INFO:root:No instance specified, using random instance https://nitter.moomoo.me\n",
            "INFO:root:Current stats for MSFT: 10 tweets, 0 threads...\n",
            "INFO:root:No instance specified, using random instance https://nitter.tinfoil-hat.net\n",
            "INFO:root:Current stats for NVDA: 8 tweets, 0 threads...\n",
            "INFO:root:Current stats for NVDA: 10 tweets, 0 threads...\n",
            "INFO:root:No instance specified, using random instance https://nitter.tinfoil-hat.net\n",
            "INFO:root:Current stats for AMZN: 3 tweets, 0 threads...\n",
            "INFO:root:Current stats for AMZN: 5 tweets, 0 threads...\n",
            "INFO:root:Current stats for AMZN: 10 tweets, 0 threads...\n",
            "INFO:root:No instance specified, using random instance https://nitter.adminforge.de\n",
            "INFO:root:Current stats for TSLA: 4 tweets, 0 threads...\n",
            "INFO:root:Current stats for TSLA: 10 tweets, 0 threads...\n",
            "INFO:root:No instance specified, using random instance https://nitter.uni-sonia.com\n",
            "INFO:root:Current stats for IBM: 2 tweets, 0 threads...\n",
            "INFO:root:Current stats for IBM: 7 tweets, 0 threads...\n",
            "INFO:root:Current stats for IBM: 10 tweets, 0 threads...\n"
          ]
        }
      ]
    }
  ]
}