# -*- coding: utf-8 -*-
"""CS410_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHXKTKqKs8yTC-M1DukeCusxVAHe_1nk
"""

#pip install ntscraper

#pip install clean-text

import pandas as pd
from ntscraper import Nitter
from cleantext import clean

scraper = Nitter()

def get_tweets(name,modes,num):
    tweets = scraper.get_tweets(name,mode = modes,number=num, language = "en", filters = ["nativeretweets"])
    final_tweets = []
    for tweet in tweets['tweets']:
        data = [tweet['link'], tweet['text'],tweet['date'],tweet['stats']['likes'],tweet['stats']['quotes'],tweet['stats']['comments'],tweet['stats']['retweets']]
        final_tweets.append(data)
    data = pd.DataFrame(final_tweets,columns= ['link','text','date','likes','quotes','comments','retweets'])

    return data

"""APPL,MSFT,AMZN,NVDA,GOOGL,META,TSLA,
Compare AI
"""

pd.set_option('display.max_colwidth', None)

data = get_tweets('META','hashtag',20)

data['text'] = data['text'].str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)
data['text'] = data['text'].apply(clean)

hashtag = ['META','MSFT',"NVDA","AMZN","TSLA","IBM"]
data = pd.DataFrame()

for i in hashtag:
    update_data = get_tweets(i,'hashtag',10)
    data = pd.concat([data, update_data], ignore_index=True)

data['text'] = data['text'].str.replace(
    r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True)
data.to_csv('tweets.csv', index=False)